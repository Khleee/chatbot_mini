{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  수정사항\n",
    "1. seed 2022 고정\n",
    "2. model은 kobert-lm\n",
    "3. train, valid, test 데이터셋에 문장들이 골고루 들어가게 설정\n",
    "    > 사전에 미리 분리해 놓음\n",
    "    > test : 라벨당 6개씩 추출(1122)\n",
    "    > train, valid : 약 4:1 비율로 뽑음 (8959 1881)\n",
    "4. early stop 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러올 수많은 모듈들..\n",
    "import json\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoConfig\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤시드 설정\n",
    "seed_val = 2022\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed(seed_val) # 멀티 gpu의 경우 뒤에 _all이 붙이면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 선언\n",
    "tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert-lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1개 존재함\n",
      "사용할 GPU :  NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# gpu 메모리 청소\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# device = gpu 설정\n",
    "if torch.cuda.is_available():     \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('%d개 존재함' % torch.cuda.device_count())\n",
    "    print('사용할 GPU : ', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('GPU 없어서 CPU로 설정')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## csv로 읽는 경우\n",
    "\n",
    "trainn = pd.read_csv(\"data/final2_train2.csv\",encoding=\"utf-8\")\n",
    "validd = pd.read_csv(\"data/final2_valid2.csv\",encoding=\"utf-8\")\n",
    "testt = pd.read_csv(\"data/final2_test2.csv\",encoding=\"utf-8\")\n",
    "\n",
    "# intent_list 생성\n",
    "B = pd.read_csv(\"data/final2_title_node(187개).csv\")\n",
    "intent_list = list(B[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>잘가</td>\n",
       "      <td>그만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bye</td>\n",
       "      <td>그만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>상담 종료</td>\n",
       "      <td>그만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>끝</td>\n",
       "      <td>그만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>잘자요</td>\n",
       "      <td>그만</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8954</th>\n",
       "      <td>이미 회원인데 어떻게 하지?</td>\n",
       "      <td>이미회원인경우</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8955</th>\n",
       "      <td>이미 회원가입해서요</td>\n",
       "      <td>이미회원인경우</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8956</th>\n",
       "      <td>벌써 가입했어요</td>\n",
       "      <td>이미회원인경우</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8957</th>\n",
       "      <td>벌써 가입했거든요</td>\n",
       "      <td>이미회원인경우</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8958</th>\n",
       "      <td>이미 가입했는데요</td>\n",
       "      <td>이미회원인경우</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8959 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              example    label\n",
       "0                  잘가       그만\n",
       "1                 bye       그만\n",
       "2               상담 종료       그만\n",
       "3                   끝       그만\n",
       "4                 잘자요       그만\n",
       "...               ...      ...\n",
       "8954  이미 회원인데 어떻게 하지?  이미회원인경우\n",
       "8955       이미 회원가입해서요  이미회원인경우\n",
       "8956         벌써 가입했어요  이미회원인경우\n",
       "8957        벌써 가입했거든요  이미회원인경우\n",
       "8958        이미 가입했는데요  이미회원인경우\n",
       "\n",
       "[8959 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "# intent_dic 생성\n",
    "intent_dic = {}\n",
    "for i,x in enumerate(intent_list):\n",
    "    intent_dic[x] = i\n",
    "\n",
    "# intent 개수\n",
    "print(len(intent_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'그만': 0,\n",
       " '실망및질책': 1,\n",
       " '일은잘하니': 2,\n",
       " '웃음': 3,\n",
       " '사과': 4,\n",
       " '감사': 5,\n",
       " '고객기분나쁨': 6,\n",
       " '고객기분좋음': 7,\n",
       " '수긍': 8,\n",
       " '노래': 9,\n",
       " '모바일쿠폰문의': 10,\n",
       " '회원정보변경': 11,\n",
       " '이름변경': 12,\n",
       " '정보메일수신거부': 13,\n",
       " '회원등급조건': 14,\n",
       " '회원최고등급': 15,\n",
       " '회원최하등급': 16,\n",
       " '회원가입혜택안내': 17,\n",
       " '회원탈퇴안내': 18,\n",
       " '회원ID분실': 19,\n",
       " '회원비밀번호분실': 20,\n",
       " '회원가입불가시': 21,\n",
       " '회원약관안내': 22,\n",
       " '탈퇴재가입가능여부': 23,\n",
       " '탈퇴재가입혜택이동': 24,\n",
       " '동원몰회원제': 25,\n",
       " '체험단이벤트안내': 26,\n",
       " '체험단배송안내': 27,\n",
       " '체험단배송비결제안내': 28,\n",
       " '체험단당첨안내': 29,\n",
       " '당첨경품배송': 30,\n",
       " '이달의이벤트안내': 31,\n",
       " '상품평혜택안내': 32,\n",
       " '현금영수증신청가능결제수단': 33,\n",
       " '현금영수증신청방법': 34,\n",
       " '현금영수증발급내역열람': 35,\n",
       " '현금영수증명의': 36,\n",
       " '발행종류': 37,\n",
       " '세금계산서요청방법': 38,\n",
       " '세금계산서금액기준': 39,\n",
       " '세금계산서공급자표시': 40,\n",
       " '세금계산서발행일자표시': 41,\n",
       " '세금계산서마감': 42,\n",
       " '세금계산서요청사업자등록증': 43,\n",
       " '제품색변화': 44,\n",
       " '제품보관법': 45,\n",
       " '레시피등록': 46,\n",
       " '고객만족센터연락처': 47,\n",
       " '고객만족센터운영시간': 48,\n",
       " '고객만족센터전화불통': 49,\n",
       " '문의내역확인': 50,\n",
       " '상품QA수정': 51,\n",
       " '직접문의안내': 52,\n",
       " '지금바로처리': 53,\n",
       " '클레임접수': 54,\n",
       " '해결불가건': 55,\n",
       " '임직원몰안내': 56,\n",
       " '임직원몰이동방법': 57,\n",
       " '임직원몰혜택': 58,\n",
       " '임직원포인트안내': 59,\n",
       " '임직원인증방법': 60,\n",
       " '회사위치': 61,\n",
       " '이달의기획전안내': 62,\n",
       " '타사문의': 63,\n",
       " '회사연락처': 64,\n",
       " '동원홈페이지': 65,\n",
       " '동원몰홈페이지': 66,\n",
       " '동원소개': 67,\n",
       " '대표이사안내': 68,\n",
       " '동원몰소개': 69,\n",
       " '동원몰앱안내': 70,\n",
       " '입점문의': 71,\n",
       " '동원몰상품문의': 72,\n",
       " '동원몰매출': 73,\n",
       " '전체카테고리안내': 74,\n",
       " '동원입사': 75,\n",
       " '임박상품': 76,\n",
       " '천지인상품추천': 77,\n",
       " '코스트코구매혜택': 78,\n",
       " '동원브랜드및상품소개': 79,\n",
       " '참치상품안내': 80,\n",
       " '동원참치원산지': 81,\n",
       " '만두상품안내': 82,\n",
       " '리챔상품안내': 83,\n",
       " '쎈쿡상품안내': 84,\n",
       " '김치상품안내': 85,\n",
       " '샘물상품안내': 86,\n",
       " '동원몰베스트상품소개': 87,\n",
       " '카테고리베스트안내-가공.즉석.냉동': 88,\n",
       " '카테고리베스트안내-유아동.도서': 89,\n",
       " '카테고리베스트안내-가전.사무.디지털': 90,\n",
       " '카테고리베스트안내-모바일쿠폰': 91,\n",
       " '카테고리베스트안내-코스트코': 92,\n",
       " '카테고리베스트안내-김치.반찬.소스': 93,\n",
       " '카테고리베스트안내-쌀.과일.채소.견과': 94,\n",
       " '카테고리베스트안내-수산': 95,\n",
       " '카테고리베스트안내-축산': 96,\n",
       " '카테고리베스트안내-건식.다이어트': 97,\n",
       " '카테고리베스트안내-생활': 98,\n",
       " '카테고리베스트안내-주방': 99,\n",
       " '카테고리베스트안내-화장품.바디.헤어': 100,\n",
       " '동원몰강점': 101,\n",
       " '농담': 102,\n",
       " '증시': 103,\n",
       " '날짜및시간': 104,\n",
       " '뭐먹었어': 105,\n",
       " '음식추천': 106,\n",
       " '뭐해': 107,\n",
       " '사랑해': 108,\n",
       " '심심해': 109,\n",
       " '시사상식': 110,\n",
       " '주문절차안내': 111,\n",
       " '주문내용확인': 112,\n",
       " '주문상품변경': 113,\n",
       " '주문후주소지변경': 114,\n",
       " '비회원주문': 115,\n",
       " '여러상품주문': 116,\n",
       " '대량주문안내': 117,\n",
       " '품절일시품절상품구입': 118,\n",
       " '상품가격차이': 119,\n",
       " '주문취소방법안내': 120,\n",
       " '결제방법안내': 121,\n",
       " '휴대폰결제안내': 122,\n",
       " '무이자신용카드': 123,\n",
       " '결제방법변경': 124,\n",
       " '입금확인안내': 125,\n",
       " '입금자명불일치안내': 126,\n",
       " '공인인증서의무사용안내': 127,\n",
       " '현금결제방법안내': 128,\n",
       " '결제지연경우안내': 129,\n",
       " '적립금안내': 130,\n",
       " '적립금결제안내': 131,\n",
       " '적립금소멸안내': 132,\n",
       " '쿠폰안내': 133,\n",
       " '쿠폰결제안내': 134,\n",
       " '쿠폰소멸안내': 135,\n",
       " '받을수있는쿠폰안내': 136,\n",
       " '적립금사용환원안내': 137,\n",
       " '쿠폰사용환원안내': 138,\n",
       " '씨앗사용환원안내': 139,\n",
       " '교환요청': 140,\n",
       " '교환정책': 141,\n",
       " '교환회수날짜지정': 142,\n",
       " '다른상품교환': 143,\n",
       " '반품요청': 144,\n",
       " '비회원반품': 145,\n",
       " '반품절차': 146,\n",
       " '일부취소반품': 147,\n",
       " '반품주소문의': 148,\n",
       " '환불요청': 149,\n",
       " '취소반품여부': 150,\n",
       " '환불소요기간': 151,\n",
       " '환불절차': 152,\n",
       " '묶음배송': 153,\n",
       " '밴드배송안내': 154,\n",
       " '쿨밴드배송안내': 155,\n",
       " '배송비부담기준안내': 156,\n",
       " '반품배송비안내': 157,\n",
       " '배송정책': 158,\n",
       " '도서산간지역배송': 159,\n",
       " '비회원배송확인': 160,\n",
       " '무료배송안내': 161,\n",
       " '배송일지정불가안내': 162,\n",
       " '배송기간안내': 163,\n",
       " '배송누락': 164,\n",
       " '상품수령후발송완료문자': 165,\n",
       " '상품문의': 166,\n",
       " '동원선물세트특판': 167,\n",
       " '양반명품김치상품권등록': 168,\n",
       " '임직원복지포인트미지급': 169,\n",
       " '쨈색변하는이유': 170,\n",
       " '신규가입혜택다운': 171,\n",
       " '로그인방법': 172,\n",
       " '택배사문의': 173,\n",
       " '참치통조림기름섭취': 174,\n",
       " 'GNC선물세트특판': 175,\n",
       " '양반명품김치상품권안내': 176,\n",
       " '아직인증못했어요': 177,\n",
       " '쎈쿡용기환경호르몬': 178,\n",
       " '리챔내용물쉽게꺼내는방법': 179,\n",
       " '천지인선물세트특판': 180,\n",
       " '회원가입안내': 181,\n",
       " '자연한입고구마하얀가루': 182,\n",
       " '동원천지인백수오사용': 183,\n",
       " '임산부참치통조림': 184,\n",
       " '어린이홍삼안전성': 185,\n",
       " '이미회원인경우': 186}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['그만',\n",
       " '실망및질책',\n",
       " '일은잘하니',\n",
       " '웃음',\n",
       " '사과',\n",
       " '감사',\n",
       " '고객기분나쁨',\n",
       " '고객기분좋음',\n",
       " '수긍',\n",
       " '노래',\n",
       " '모바일쿠폰문의',\n",
       " '회원정보변경',\n",
       " '이름변경',\n",
       " '정보메일수신거부',\n",
       " '회원등급조건',\n",
       " '회원최고등급',\n",
       " '회원최하등급',\n",
       " '회원가입혜택안내',\n",
       " '회원탈퇴안내',\n",
       " '회원ID분실',\n",
       " '회원비밀번호분실',\n",
       " '회원가입불가시',\n",
       " '회원약관안내',\n",
       " '탈퇴재가입가능여부',\n",
       " '탈퇴재가입혜택이동',\n",
       " '동원몰회원제',\n",
       " '체험단이벤트안내',\n",
       " '체험단배송안내',\n",
       " '체험단배송비결제안내',\n",
       " '체험단당첨안내',\n",
       " '당첨경품배송',\n",
       " '이달의이벤트안내',\n",
       " '상품평혜택안내',\n",
       " '현금영수증신청가능결제수단',\n",
       " '현금영수증신청방법',\n",
       " '현금영수증발급내역열람',\n",
       " '현금영수증명의',\n",
       " '발행종류',\n",
       " '세금계산서요청방법',\n",
       " '세금계산서금액기준',\n",
       " '세금계산서공급자표시',\n",
       " '세금계산서발행일자표시',\n",
       " '세금계산서마감',\n",
       " '세금계산서요청사업자등록증',\n",
       " '제품색변화',\n",
       " '제품보관법',\n",
       " '레시피등록',\n",
       " '고객만족센터연락처',\n",
       " '고객만족센터운영시간',\n",
       " '고객만족센터전화불통',\n",
       " '문의내역확인',\n",
       " '상품QA수정',\n",
       " '직접문의안내',\n",
       " '지금바로처리',\n",
       " '클레임접수',\n",
       " '해결불가건',\n",
       " '임직원몰안내',\n",
       " '임직원몰이동방법',\n",
       " '임직원몰혜택',\n",
       " '임직원포인트안내',\n",
       " '임직원인증방법',\n",
       " '회사위치',\n",
       " '이달의기획전안내',\n",
       " '타사문의',\n",
       " '회사연락처',\n",
       " '동원홈페이지',\n",
       " '동원몰홈페이지',\n",
       " '동원소개',\n",
       " '대표이사안내',\n",
       " '동원몰소개',\n",
       " '동원몰앱안내',\n",
       " '입점문의',\n",
       " '동원몰상품문의',\n",
       " '동원몰매출',\n",
       " '전체카테고리안내',\n",
       " '동원입사',\n",
       " '임박상품',\n",
       " '천지인상품추천',\n",
       " '코스트코구매혜택',\n",
       " '동원브랜드및상품소개',\n",
       " '참치상품안내',\n",
       " '동원참치원산지',\n",
       " '만두상품안내',\n",
       " '리챔상품안내',\n",
       " '쎈쿡상품안내',\n",
       " '김치상품안내',\n",
       " '샘물상품안내',\n",
       " '동원몰베스트상품소개',\n",
       " '카테고리베스트안내-가공.즉석.냉동',\n",
       " '카테고리베스트안내-유아동.도서',\n",
       " '카테고리베스트안내-가전.사무.디지털',\n",
       " '카테고리베스트안내-모바일쿠폰',\n",
       " '카테고리베스트안내-코스트코',\n",
       " '카테고리베스트안내-김치.반찬.소스',\n",
       " '카테고리베스트안내-쌀.과일.채소.견과',\n",
       " '카테고리베스트안내-수산',\n",
       " '카테고리베스트안내-축산',\n",
       " '카테고리베스트안내-건식.다이어트',\n",
       " '카테고리베스트안내-생활',\n",
       " '카테고리베스트안내-주방',\n",
       " '카테고리베스트안내-화장품.바디.헤어',\n",
       " '동원몰강점',\n",
       " '농담',\n",
       " '증시',\n",
       " '날짜및시간',\n",
       " '뭐먹었어',\n",
       " '음식추천',\n",
       " '뭐해',\n",
       " '사랑해',\n",
       " '심심해',\n",
       " '시사상식',\n",
       " '주문절차안내',\n",
       " '주문내용확인',\n",
       " '주문상품변경',\n",
       " '주문후주소지변경',\n",
       " '비회원주문',\n",
       " '여러상품주문',\n",
       " '대량주문안내',\n",
       " '품절일시품절상품구입',\n",
       " '상품가격차이',\n",
       " '주문취소방법안내',\n",
       " '결제방법안내',\n",
       " '휴대폰결제안내',\n",
       " '무이자신용카드',\n",
       " '결제방법변경',\n",
       " '입금확인안내',\n",
       " '입금자명불일치안내',\n",
       " '공인인증서의무사용안내',\n",
       " '현금결제방법안내',\n",
       " '결제지연경우안내',\n",
       " '적립금안내',\n",
       " '적립금결제안내',\n",
       " '적립금소멸안내',\n",
       " '쿠폰안내',\n",
       " '쿠폰결제안내',\n",
       " '쿠폰소멸안내',\n",
       " '받을수있는쿠폰안내',\n",
       " '적립금사용환원안내',\n",
       " '쿠폰사용환원안내',\n",
       " '씨앗사용환원안내',\n",
       " '교환요청',\n",
       " '교환정책',\n",
       " '교환회수날짜지정',\n",
       " '다른상품교환',\n",
       " '반품요청',\n",
       " '비회원반품',\n",
       " '반품절차',\n",
       " '일부취소반품',\n",
       " '반품주소문의',\n",
       " '환불요청',\n",
       " '취소반품여부',\n",
       " '환불소요기간',\n",
       " '환불절차',\n",
       " '묶음배송',\n",
       " '밴드배송안내',\n",
       " '쿨밴드배송안내',\n",
       " '배송비부담기준안내',\n",
       " '반품배송비안내',\n",
       " '배송정책',\n",
       " '도서산간지역배송',\n",
       " '비회원배송확인',\n",
       " '무료배송안내',\n",
       " '배송일지정불가안내',\n",
       " '배송기간안내',\n",
       " '배송누락',\n",
       " '상품수령후발송완료문자',\n",
       " '상품문의',\n",
       " '동원선물세트특판',\n",
       " '양반명품김치상품권등록',\n",
       " '임직원복지포인트미지급',\n",
       " '쨈색변하는이유',\n",
       " '신규가입혜택다운',\n",
       " '로그인방법',\n",
       " '택배사문의',\n",
       " '참치통조림기름섭취',\n",
       " 'GNC선물세트특판',\n",
       " '양반명품김치상품권안내',\n",
       " '아직인증못했어요',\n",
       " '쎈쿡용기환경호르몬',\n",
       " '리챔내용물쉽게꺼내는방법',\n",
       " '천지인선물세트특판',\n",
       " '회원가입안내',\n",
       " '자연한입고구마하얀가루',\n",
       " '동원천지인백수오사용',\n",
       " '임산부참치통조림',\n",
       " '어린이홍삼안전성',\n",
       " '이미회원인경우']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_data을 읽어서 [[\"감사\",굿],[]... ] 이런꼴로 만들자\n",
    "# 그리고 \"감사\" -> dic에 맞게 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "train_content = []\n",
    "valid_content = []\n",
    "test_content = []\n",
    "\n",
    "\n",
    "for x in trainn.iloc:\n",
    "    if x[\"label\"] in intent_list:\n",
    "        train_content.append([tokenizer.tokenize(x[\"example\"]),intent_dic[x[\"label\"]]])\n",
    "\n",
    "for x in validd.iloc:\n",
    "    if x[\"label\"] in intent_list:\n",
    "        valid_content.append([tokenizer.tokenize(x[\"example\"]),intent_dic[x[\"label\"]]])\n",
    "\n",
    "for x in testt.iloc:\n",
    "    if x[\"label\"] in intent_list:\n",
    "        test_content.append([tokenizer.tokenize(x[\"example\"]),intent_dic[x[\"label\"]]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁상담', '▁종료'], 0]\n",
      "[['▁상담', '▁종료', '해', '줘'], 0]\n",
      "[['▁바', '이'], 0]\n"
     ]
    }
   ],
   "source": [
    "# 요소 하나 확인\n",
    "print(train_content[2])\n",
    "print(valid_content[2])\n",
    "print(test_content[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어, 개체명 종류로 리스트 2개 분리\n",
    "train_word = [x[0] for x in train_content]\n",
    "train_num = [x[1] for x in train_content]\n",
    "\n",
    "valid_word = [x[0] for x in valid_content]\n",
    "valid_num = [x[1] for x in valid_content]\n",
    "\n",
    "test_word = [x[0] for x in test_content]\n",
    "test_num = [x[1] for x in test_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 길이 현황 확인 + max_len으로 고정\n",
    "max_len = max(len(x) for x in token_word)\n",
    "\n",
    "plt.hist([len(x) for x in token_word], bins=50)\n",
    "plt.xlabel('length of sample')\n",
    "plt.ylabel('number of sample')\n",
    "plt.show()\n",
    "\n",
    "print('최대 길이 : %d' % max_len)\n",
    "print('최소 길이 : %d' % min(len(x) for x in token_word))\n",
    "print('평균 길이 : %f' % (sum(map(len, token_word))/len(token_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, attention 만들기\n",
    "input_ids_train = []\n",
    "attention_masks_train = []\n",
    "\n",
    "input_ids_valid = []\n",
    "attention_masks_valid = []\n",
    "\n",
    "input_ids_test = []\n",
    "attention_masks_test = []\n",
    "\n",
    "for x in train_word:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens = True, # 이미 스페셜 토큰 넣긴했지만 일단 설정함\n",
    "        max_length = max_len, # 최대길이\n",
    "        truncation = True, # 넘어가면 잘라내기\n",
    "        padding = \"max_length\", # 최대길이 이상은 패딩\n",
    "        return_attention_mask = True, # 어텐션 마스크 설정\n",
    "        return_tensors = \"pt\" # 파이토치 텐서로 출력\n",
    "    )\n",
    "\n",
    "    input_ids_train.append(encoded_dict[\"input_ids\"]) # ids\n",
    "    attention_masks_train.append(encoded_dict[\"attention_mask\"]) # attention mask\n",
    "\n",
    "for x in valid_word:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens = True, # 이미 스페셜 토큰 넣긴했지만 일단 설정함\n",
    "        max_length = max_len, # 최대길이\n",
    "        truncation = True, # 넘어가면 잘라내기\n",
    "        padding = \"max_length\", # 최대길이 이상은 패딩\n",
    "        return_attention_mask = True, # 어텐션 마스크 설정\n",
    "        return_tensors = \"pt\" # 파이토치 텐서로 출력\n",
    "    )\n",
    "\n",
    "    input_ids_valid.append(encoded_dict[\"input_ids\"]) # ids\n",
    "    attention_masks_valid.append(encoded_dict[\"attention_mask\"]) # attention mask\n",
    "\n",
    "for x in test_word:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens = True, # 이미 스페셜 토큰 넣긴했지만 일단 설정함\n",
    "        max_length = max_len, # 최대길이\n",
    "        truncation = True, # 넘어가면 잘라내기\n",
    "        padding = \"max_length\", # 최대길이 이상은 패딩\n",
    "        return_attention_mask = True, # 어텐션 마스크 설정\n",
    "        return_tensors = \"pt\" # 파이토치 텐서로 출력\n",
    "    )\n",
    "\n",
    "    input_ids_test.append(encoded_dict[\"input_ids\"]) # ids\n",
    "    attention_masks_test.append(encoded_dict[\"attention_mask\"]) # attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 모양 : tensor([[   2, 3942, 5330,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]])\n",
      "크기 : torch.Size([1, 124])\n",
      "개수 : 8959\n",
      "==================\n",
      "0번째 모양 : tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]])\n",
      "크기 : torch.Size([1, 124])\n",
      "개수 : 8959\n",
      "==================\n",
      "0번째 모양 : 0\n",
      "개수 : 8959\n"
     ]
    }
   ],
   "source": [
    "print(\"0번째 모양 :\", input_ids_train[0])\n",
    "print(\"크기 :\", input_ids_train[0].size())\n",
    "print(\"개수 :\",len(input_ids_train))\n",
    "print(\"==================\")\n",
    "print(\"0번째 모양 :\", attention_masks_train[0])\n",
    "print(\"크기 :\",attention_masks_train[0].size())\n",
    "print(\"개수 :\",len(attention_masks_train))\n",
    "print(\"==================\")\n",
    "print(\"0번째 모양 :\", train_num[0])\n",
    "print(\"개수 :\",len(train_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122 8959 1881\n",
      "11962\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids_test),len(input_ids_train),len(input_ids_valid))\n",
    "print(len(input_ids_test)+len(input_ids_train)+len(input_ids_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1122 1122 1122\n",
      "8959 8959 8959\n",
      "1881 1881 1122\n"
     ]
    }
   ],
   "source": [
    "# train, valid, test 별로 각 데이터가 제대로 있는 지 확인\n",
    "print(len(input_ids_test),len(attention_masks_test),len(test_num))\n",
    "print(len(input_ids_train),len(attention_masks_train),len(train_num))\n",
    "print(len(input_ids_valid),len(attention_masks_valid),len(test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서들로 이루어져있는 리스트 -> 하나의 텐서로 만들기2\n",
    "input_ids_train = torch.cat(input_ids_train, dim=0) #  dim = 0 으로 전환\n",
    "attention_masks_train = torch.cat(attention_masks_train, dim=0) #  dim = 0 으로 전환\n",
    "labels_train = torch.tensor(train_num) # 한줄 리스트라 이미 dim=0임\n",
    "\n",
    "input_ids_valid = torch.cat(input_ids_valid, dim=0) #  dim = 0 으로 전환\n",
    "attention_masks_valid = torch.cat(attention_masks_valid, dim=0) #  dim = 0 으로 전환\n",
    "labels_valid = torch.tensor(valid_num) # 한줄 리스트라 이미 dim=0임\n",
    "\n",
    "input_ids_test = torch.cat(input_ids_test, dim=0) #  dim = 0 으로 전환\n",
    "attention_masks_test = torch.cat(attention_masks_test, dim=0) #  dim = 0 으로 전환\n",
    "labels_test = torch.tensor(test_num) # 한줄 리스트라 이미 dim=0임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset : 8959\n",
      "val_dataset : 1881\n",
      "test_dataset : 1122\n",
      "전체 dataset : 11962\n"
     ]
    }
   ],
   "source": [
    "# tensordataset 모듈을 사용해서 합치기\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "# 마지막으로 제대로 개수가 들어 있는지 확인\n",
    "print(f\"train_dataset : {len(train_dataset)}\")\n",
    "print(f\"val_dataset : {len(val_dataset)}\")\n",
    "print(f\"test_dataset : {len(test_dataset)}\")\n",
    "print(f\"전체 dataset : {len(train_dataset)+len(val_dataset)+len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기\n",
    "model_name = \"monologg/kobert-lm\"\n",
    "\n",
    "# 모델 설정 불러오기\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "# 모델 라벨 개수 지정\n",
    "config.num_labels = len(intent_dic)\n",
    "\n",
    "# 모델에 attentions 존재함\n",
    "config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 설정 확인하려면\n",
    "# print(\"config\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/kobert-lm were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=187, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 불러오기(모델+설정)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config)\n",
    "\n",
    "# 쿠다 사용할거야\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의로 배치 사이즈 지정\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 로더 -> 배치사이즈에 맞게 잘 섞고 iter 형태로 전환\n",
    "# valid, test는 굳이 섞을 필요가 없음(고정되있는게 오히려 비교하기 좋음)\n",
    "# train\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# valid\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# test\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YU\\miniconda3\\envs\\py38\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# adamw 라는 새로운 옵티마이저 사용함\n",
    "# https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html\n",
    "# 해보니깐 adam 도 마냥 만능이 아니라서 여러가지로 개선할 방법을 강구하는 과정에서 나온 것들\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 5e-5, # 학습률\n",
    "                  eps = 1e-8 # 아담 엡실론\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# train_Dataloader 개수 확인\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(train_dataset)\u001b[39m/\u001b[39mbatch_size , \u001b[39mlen\u001b[39m(train_dataloader))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# train_Dataloader 개수 확인\n",
    "# train_dataset / batch_size = 548.4... = 549 = len(train_dataloader) = train step 수 \n",
    "\n",
    "print(len(train_dataset)/batch_size , len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복 횟수 지정\n",
    "epochs = 40\n",
    "\n",
    "# 1 epoch당 train 기준 351번 step 밟음 -> 10epoch = 3510\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 학습률 줄이는데에 시간이 오래걸리므로 처음 lr크기를 크게준후 epoch 마다 값을 감소시켜서 빠르게 도달하기\n",
    "# 아니면 학습률 증가 -> 학습률 감소 식으로도 구현 가능\n",
    "# cosine , linear, inverse_sqrt, constant ....\n",
    "\n",
    "# 그중에서 우리는 1-cycle 스케줄링을 사용함 => linear하게 증가, 감소함.(옵티마이저는 그대로, )\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# 1-cycle 스케줄링의 경우\n",
    "# num_warmup_steps : 이 step수에 해당할때까지 0부터 선형적으로 증가\n",
    "# ex) 100 이면, 100에 이를때까지 0부터 linear하게 증가\n",
    "# 100에 도달하고 그 이후부터는 linear하게 감소\n",
    "# 그래서 일반적으로 train_dataloader 개수에 1/10 으로 지정하는 등으로 함\n",
    "# # num_training_steps : 전체 step 수\n",
    "\n",
    "## 저기서 num_warmup_steps 조정하면 여러번 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제값을이용하여 예측값의 정확도를 계산하는 함수 선언\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() # 한줄로 나열\n",
    "    labels_flat = labels.flatten() # 한줄로 나열2\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    # (틀리면0, 맞으면 1/전체 개수)의 총합 = ac 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분포가 불균형적이면, f1_score\n",
    "# 데이터 분포가 균형적이면, acc\n",
    "\n",
    "# 궁극적으로 구현해보고 싶은거\n",
    "# warm_steps 조절해서 성능에 미치는 영향\n",
    "# f1_score(micro)로 계산하게 동작\n",
    "# test시 전체 분류표도 출력되게 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간을 재봐요 함수\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # 반올림해서 정수로 표현\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # 시간/분/초로 출력되게 하자\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:17.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:33.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:48.\n",
      "  Batch   200  of    280.    Elapsed: 0:01:04.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:19.\n",
      "\n",
      "  평균 training loss: 3.78\n",
      "  Training 1 epoch 걸린 시간: 0:01:29\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "  Validation Loss: 2.41\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 2 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:16.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:31.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:47.\n",
      "  Batch   200  of    280.    Elapsed: 0:01:02.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:18.\n",
      "\n",
      "  평균 training loss: 1.69\n",
      "  Training 1 epoch 걸린 시간: 0:01:27\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.85\n",
      "  Validation Loss: 1.09\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "save model's epochs : 2\n",
      "\n",
      "======== Epoch 3 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:15.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:31.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:46.\n",
      "  Batch   200  of    280.    Elapsed: 0:01:01.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:17.\n",
      "\n",
      "  평균 training loss: 0.75\n",
      "  Training 1 epoch 걸린 시간: 0:01:26\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 3\n",
      "\n",
      "======== Epoch 4 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:15.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:29.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:58.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:12.\n",
      "\n",
      "  평균 training loss: 0.38\n",
      "  Training 1 epoch 걸린 시간: 0:01:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation Loss: 0.42\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 4\n",
      "\n",
      "======== Epoch 5 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:42.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:56.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:10.\n",
      "\n",
      "  평균 training loss: 0.21\n",
      "  Training 1 epoch 걸린 시간: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.30\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 5\n",
      "\n",
      "======== Epoch 6 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:42.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:57.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.12\n",
      "  Training 1 epoch 걸린 시간: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.24\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 6\n",
      "\n",
      "======== Epoch 7 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:42.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:56.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.07\n",
      "  Training 1 epoch 걸린 시간: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  Validation Loss: 0.23\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 7\n",
      "\n",
      "======== Epoch 8 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:42.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:56.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.05\n",
      "  Training 1 epoch 걸린 시간: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 8\n",
      "\n",
      "======== Epoch 9 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:42.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:57.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.03\n",
      "  Training 1 epoch 걸린 시간: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 9\n",
      "\n",
      "======== Epoch 10 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:42.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:57.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.02\n",
      "  Training 1 epoch 걸린 시간: 0:01:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Warning! patience : 2\n",
      "\n",
      "======== Epoch 11 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:29.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:43.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:57.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.02\n",
      "  Training 1 epoch 걸린 시간: 0:01:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 11\n",
      "\n",
      "======== Epoch 12 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:28.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:43.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:57.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:11.\n",
      "\n",
      "  평균 training loss: 0.02\n",
      "  Training 1 epoch 걸린 시간: 0:01:20\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 12\n",
      "\n",
      "======== Epoch 13 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:14.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:29.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:58.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:13.\n",
      "\n",
      "  평균 training loss: 0.01\n",
      "  Training 1 epoch 걸린 시간: 0:01:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "save model's epochs : 13\n",
      "\n",
      "======== Epoch 14 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:15.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:29.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:59.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:13.\n",
      "\n",
      "  평균 training loss: 0.01\n",
      "  Training 1 epoch 걸린 시간: 0:01:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Warning! patience : 2\n",
      "\n",
      "======== Epoch 15 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:15.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:29.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:59.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:14.\n",
      "\n",
      "  평균 training loss: 0.01\n",
      "  Training 1 epoch 걸린 시간: 0:01:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.22\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Warning! patience : 1\n",
      "\n",
      "======== Epoch 16 / 40 ========\n",
      "Training...\n",
      "  Batch    50  of    280.    Elapsed: 0:00:15.\n",
      "  Batch   100  of    280.    Elapsed: 0:00:30.\n",
      "  Batch   150  of    280.    Elapsed: 0:00:44.\n",
      "  Batch   200  of    280.    Elapsed: 0:00:59.\n",
      "  Batch   250  of    280.    Elapsed: 0:01:14.\n",
      "\n",
      "  평균 training loss: 0.01\n",
      "  Training 1 epoch 걸린 시간: 0:01:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:00:05\n",
      "\n",
      "Warning! patience : 0\n",
      "\n",
      "epochs Training complete!\n",
      "Total training 시간 0:23:13 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# train,valid loss 와 valid acc 와 타임 로그도 저장할거야\n",
    "training_stats = []\n",
    "\n",
    "# 전체 걸린 시간 측정\n",
    "total_t0 = time.time()\n",
    "\n",
    "# valid loss가 학습할수록 줄어들면 동작  \n",
    "patience = 3\n",
    "first = 0 # 처음만 동작되게\n",
    "\n",
    "# 에포크 마다 돌려돌려\n",
    "for epoch_i in range(0, epochs):\n",
    "    ## train ##\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Train!')\n",
    "\n",
    "    # 1에포크당 걸린 시간 측정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 전체 손실 값 초기화\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # train 모드로 전환한다는 뜻임(학습시작이 아님)\n",
    "    model.train() \n",
    "\n",
    "    # batch 마다 돌려돌려\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # 50 스텝마다 \n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # 처음 시작에서 지나간 시간을 측정\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # 과정 출력\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 순서대로 ids, mask, labels이 담겨있음(batch 하나에)\n",
    "        b_input_ids = batch[0].to(device) \n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # 미분 기울기는 항상 초기화해야해\n",
    "        # 안그러면 학습할때마다 계속 기존의 미분 기울기에 적용되서 이상해져버려\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 모델에 돌릴거야(순전파)\n",
    "        M = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = M.loss\n",
    "        logits = M.logits\n",
    "        attentions = M.attentions\n",
    "\n",
    "        # 나오는 loss값을 다 전체 손실값에 더할거야\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # 반대로 돌아가서 기울기를 역 계산하기(역전파)\n",
    "        loss.backward()\n",
    "\n",
    "        # 출력 길이에 따라 기울기 크기가 너무 커질수도 있으므로 해결하고자 하는 방법 중 하나\n",
    "        # 쉬운 방법은 학습률을 작게 취하기(길이가 달라지므로 그에 맞는 학습률이 동적으로 변화 -> adam)'Adam\n",
    "        # 그러나 일단 혹시 오버되는 일말의 문제에도 대처하기 위해서 작성함\n",
    "\n",
    "        # 기울기 벡터 방향은 유지 + 크기만 적절하게 조절((1)보다 크면 동작함)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 역전파에서 계산한 기울기들을 각각 파라미터에 적용\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습률도 스케줄러에 설정한 옵션에 맞게 조정\n",
    "        scheduler.step()\n",
    "\n",
    "    # 1에포크마다 평균 train_loss를 구함(loss 총합/전체개수)\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # 1에포크까지 나온 시간 측정\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  평균 training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training 1 epoch 걸린 시간: {:}\".format(training_time))\n",
    "    \n",
    "    ## valid ##\n",
    "    # 학습이 다 끝난후 valid 데이터 가지고 성능이 어느정도인지 확인\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Valid!\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 평가 모드로 전환(+ dropout, batchnorm 비활성화해줌)\n",
    "    # 메모리는 그대로\n",
    "    model.eval()\n",
    "\n",
    "    # torch.no_grad()을 사용하면 자동미분 계산기능을 아예 꺼버려서 메모리 줄이고 연산속도가 증가함\n",
    "    # 대신 dropout 같은 요소들을 비활성화 하지 않음\n",
    "    # 그래서 model.eval()로 드랍아웃 끄고, torch.no_grad()을 씌워서 메모리도 줄이는 형태로 되어있음\n",
    "\n",
    "    # 변수들을 추적하자\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # 1에포크 시점\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # 순전파 돌린 값만을 보고 측정하기 떄문에 autograd 엔진 끔\n",
    "        with torch.no_grad():   \n",
    "            M = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "            loss = M.loss\n",
    "            logits = M.logits\n",
    "            attentions = M.attentions\n",
    "            \n",
    "        # valid loss 값 다 더해\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # 결과 logits, 결과 라벨 -> cpu에 할당\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # 전체 acc 값에다 acc 계산 함수값을 계속 더해\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # 평균 acc 값을 구해서 출력\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # 1에포크에 걸처 나온 전체 loss 값을 평균화 하여 출력\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # valid 얼마나 걸렸는지 출력\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # 통계 수치들을 다 저장해놓자\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    # early stopping -> 자동으로 제일 나은 모델만 저장함\n",
    "    if first == 0:\n",
    "        before_valid = training_stats[-1][\"Valid. Loss\"]\n",
    "        first = 1\n",
    "    elif first == 1:\n",
    "        if before_valid > training_stats[-1][\"Valid. Loss\"]:\n",
    "            patience = 3\n",
    "            before_valid = training_stats[-1][\"Valid. Loss\"]\n",
    "            save_dir = \"model/kobertLM_new_5\" ## 자동으로 저장\n",
    "            model.save_pretrained(save_dir)\n",
    "            print(\"\")\n",
    "            print(\"save model's epochs : %d\" % (epoch_i+1))\n",
    "        elif before_valid <= training_stats[-1][\"Valid. Loss\"]:\n",
    "            patience -= 1\n",
    "            print(\"\")\n",
    "            print(\"Warning! patience : %d\" % patience)\n",
    "    \n",
    "    if patience == 0:\n",
    "        break\n",
    "\n",
    "print(\"\")\n",
    "print(\"epochs Training complete!\")\n",
    "\n",
    "print(\"Total training 시간 {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.780355</td>\n",
       "      <td>2.412683</td>\n",
       "      <td>0.656674</td>\n",
       "      <td>0:01:29</td>\n",
       "      <td>0:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.685537</td>\n",
       "      <td>1.089495</td>\n",
       "      <td>0.846038</td>\n",
       "      <td>0:01:27</td>\n",
       "      <td>0:00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.749111</td>\n",
       "      <td>0.619466</td>\n",
       "      <td>0.903983</td>\n",
       "      <td>0:01:26</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380501</td>\n",
       "      <td>0.416912</td>\n",
       "      <td>0.932055</td>\n",
       "      <td>0:01:20</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.213866</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>0.946886</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.123768</td>\n",
       "      <td>0.240164</td>\n",
       "      <td>0.954831</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.074377</td>\n",
       "      <td>0.231157</td>\n",
       "      <td>0.953771</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.046147</td>\n",
       "      <td>0.198251</td>\n",
       "      <td>0.960657</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.029958</td>\n",
       "      <td>0.190963</td>\n",
       "      <td>0.962246</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.021855</td>\n",
       "      <td>0.194323</td>\n",
       "      <td>0.962246</td>\n",
       "      <td>0:01:20</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.190888</td>\n",
       "      <td>0.963305</td>\n",
       "      <td>0:01:20</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.015143</td>\n",
       "      <td>0.190104</td>\n",
       "      <td>0.965953</td>\n",
       "      <td>0:01:20</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011555</td>\n",
       "      <td>0.189668</td>\n",
       "      <td>0.964364</td>\n",
       "      <td>0:01:22</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010115</td>\n",
       "      <td>0.195217</td>\n",
       "      <td>0.962775</td>\n",
       "      <td>0:01:22</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.012433</td>\n",
       "      <td>0.217880</td>\n",
       "      <td>0.958008</td>\n",
       "      <td>0:01:22</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.008411</td>\n",
       "      <td>0.196214</td>\n",
       "      <td>0.961186</td>\n",
       "      <td>0:01:22</td>\n",
       "      <td>0:00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1           3.780355     2.412683       0.656674       0:01:29         0:00:06\n",
       "2           1.685537     1.089495       0.846038       0:01:27         0:00:06\n",
       "3           0.749111     0.619466       0.903983       0:01:26         0:00:05\n",
       "4           0.380501     0.416912       0.932055       0:01:20         0:00:05\n",
       "5           0.213866     0.296982       0.946886       0:01:19         0:00:05\n",
       "6           0.123768     0.240164       0.954831       0:01:19         0:00:05\n",
       "7           0.074377     0.231157       0.953771       0:01:19         0:00:05\n",
       "8           0.046147     0.198251       0.960657       0:01:19         0:00:05\n",
       "9           0.029958     0.190963       0.962246       0:01:19         0:00:05\n",
       "10          0.021855     0.194323       0.962246       0:01:20         0:00:05\n",
       "11          0.016802     0.190888       0.963305       0:01:20         0:00:05\n",
       "12          0.015143     0.190104       0.965953       0:01:20         0:00:05\n",
       "13          0.011555     0.189668       0.964364       0:01:22         0:00:05\n",
       "14          0.010115     0.195217       0.962775       0:01:22         0:00:05\n",
       "15          0.012433     0.217880       0.958008       0:01:22         0:00:05\n",
       "16          0.008411     0.196214       0.961186       0:01:22         0:00:05"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd로 데이터셋 만들기\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# 칼럼 명칭 지정\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# seaborn 이용해서 그래프 출력\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# 폰트 크기\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# 그림 크기\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# loss 출력\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# 축 이름 지정\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load해서 쓸거면\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"model/kobertLM_new_5\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels ...\n",
      "Predicted 1122 samples\n"
     ]
    }
   ],
   "source": [
    "# 테스트 셋에 대하여 평가해봐요\n",
    "\n",
    "print(\"Predict!\")\n",
    "\n",
    "# 평가 모드로 진입\n",
    "model2.eval()\n",
    "\n",
    "# 추적할 변수 선언\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # batch에 있는 하나하나에 gpu 할당을 해줌 + batch 변수에 다시 저장\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # batch 안에 들어있는 ids, mask, labels들을 다 끄집어 냄\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "  # 순전파만 돌리는거니 autograd 끔\n",
    "  with torch.no_grad():\n",
    "      outputs = model(input_ids=b_input_ids,\n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0] # 0번째는 logits가 들어있음\n",
    "\n",
    "  # logits, label -> cpu로 할당\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # 추적할 변수들안에 계속 차곡차곡 쌓기\n",
    "  predictions.extend(logits)\n",
    "  true_labels.extend(label_ids)\n",
    "\n",
    "print(f\"Predicted {len(predictions)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.9563279857397504\n",
      "Test set Matthews correlation coefficient: 0.9561351885317081\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(predictions, axis=1)\n",
    "print(f\"Test set accuracy: {accuracy_score(true_labels, predictions)}\")\n",
    "\n",
    "# f1_score은 사용하는 혼동 행렬들이 적기 때문에 모든 혼동행렬 종류들이 반영된 점수를 쓰는게 더 좋을 수 있다\n",
    "# ㄴ 0,1을 구별하는 이진 분류에서의 이야기임\n",
    "print(f\"Test set Matthews correlation coefficient: {matthews_corrcoef(true_labels, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### f1score을 재보자\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9563279857397504\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(true_labels, predictions, average=\"micro\")) # 동일하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(intent_dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.83      0.83      0.83         6\n",
      "           3       1.00      1.00      1.00         6\n",
      "           4       1.00      0.83      0.91         6\n",
      "           5       0.86      1.00      0.92         6\n",
      "           6       1.00      1.00      1.00         6\n",
      "           7       1.00      0.83      0.91         6\n",
      "           8       0.67      1.00      0.80         6\n",
      "           9       1.00      1.00      1.00         6\n",
      "          10       1.00      0.67      0.80         6\n",
      "          11       1.00      1.00      1.00         6\n",
      "          12       1.00      1.00      1.00         6\n",
      "          13       1.00      1.00      1.00         6\n",
      "          14       0.86      1.00      0.92         6\n",
      "          15       1.00      1.00      1.00         6\n",
      "          16       1.00      1.00      1.00         6\n",
      "          17       1.00      1.00      1.00         6\n",
      "          18       1.00      1.00      1.00         6\n",
      "          19       1.00      1.00      1.00         6\n",
      "          20       1.00      1.00      1.00         6\n",
      "          21       1.00      1.00      1.00         6\n",
      "          22       1.00      1.00      1.00         6\n",
      "          23       1.00      1.00      1.00         6\n",
      "          24       1.00      1.00      1.00         6\n",
      "          25       1.00      0.83      0.91         6\n",
      "          26       1.00      1.00      1.00         6\n",
      "          27       1.00      1.00      1.00         6\n",
      "          28       1.00      1.00      1.00         6\n",
      "          29       1.00      1.00      1.00         6\n",
      "          30       1.00      1.00      1.00         6\n",
      "          31       1.00      1.00      1.00         6\n",
      "          32       1.00      1.00      1.00         6\n",
      "          33       1.00      1.00      1.00         6\n",
      "          34       1.00      1.00      1.00         6\n",
      "          35       0.86      1.00      0.92         6\n",
      "          36       1.00      1.00      1.00         6\n",
      "          37       1.00      1.00      1.00         6\n",
      "          38       0.86      1.00      0.92         6\n",
      "          39       1.00      1.00      1.00         6\n",
      "          40       1.00      1.00      1.00         6\n",
      "          41       1.00      1.00      1.00         6\n",
      "          42       1.00      1.00      1.00         6\n",
      "          43       1.00      1.00      1.00         6\n",
      "          44       1.00      1.00      1.00         6\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       1.00      1.00      1.00         6\n",
      "          47       0.67      1.00      0.80         6\n",
      "          48       1.00      1.00      1.00         6\n",
      "          49       1.00      0.83      0.91         6\n",
      "          50       1.00      1.00      1.00         6\n",
      "          51       1.00      1.00      1.00         6\n",
      "          52       0.83      0.83      0.83         6\n",
      "          53       1.00      0.83      0.91         6\n",
      "          54       1.00      1.00      1.00         6\n",
      "          55       0.40      0.33      0.36         6\n",
      "          56       0.83      0.83      0.83         6\n",
      "          57       0.83      0.83      0.83         6\n",
      "          58       1.00      0.83      0.91         6\n",
      "          59       1.00      0.83      0.91         6\n",
      "          60       0.86      1.00      0.92         6\n",
      "          61       0.83      0.83      0.83         6\n",
      "          62       1.00      0.83      0.91         6\n",
      "          63       0.86      1.00      0.92         6\n",
      "          64       0.80      0.67      0.73         6\n",
      "          65       1.00      0.83      0.91         6\n",
      "          66       0.83      0.83      0.83         6\n",
      "          67       1.00      0.83      0.91         6\n",
      "          68       1.00      1.00      1.00         6\n",
      "          69       1.00      1.00      1.00         6\n",
      "          70       0.86      1.00      0.92         6\n",
      "          71       1.00      1.00      1.00         6\n",
      "          72       1.00      1.00      1.00         6\n",
      "          73       1.00      1.00      1.00         6\n",
      "          74       1.00      0.83      0.91         6\n",
      "          75       1.00      0.83      0.91         6\n",
      "          76       1.00      1.00      1.00         6\n",
      "          77       1.00      1.00      1.00         6\n",
      "          78       1.00      1.00      1.00         6\n",
      "          79       1.00      0.83      0.91         6\n",
      "          80       0.67      1.00      0.80         6\n",
      "          81       1.00      0.67      0.80         6\n",
      "          82       1.00      1.00      1.00         6\n",
      "          83       1.00      1.00      1.00         6\n",
      "          84       1.00      1.00      1.00         6\n",
      "          85       1.00      0.83      0.91         6\n",
      "          86       1.00      1.00      1.00         6\n",
      "          87       1.00      1.00      1.00         6\n",
      "          88       1.00      1.00      1.00         6\n",
      "          89       1.00      0.67      0.80         6\n",
      "          90       1.00      1.00      1.00         6\n",
      "          91       0.75      1.00      0.86         6\n",
      "          92       1.00      1.00      1.00         6\n",
      "          93       0.80      0.67      0.73         6\n",
      "          94       1.00      1.00      1.00         6\n",
      "          95       1.00      0.83      0.91         6\n",
      "          96       1.00      1.00      1.00         6\n",
      "          97       0.75      1.00      0.86         6\n",
      "          98       1.00      1.00      1.00         6\n",
      "          99       1.00      1.00      1.00         6\n",
      "         100       1.00      1.00      1.00         6\n",
      "         101       1.00      1.00      1.00         6\n",
      "         102       1.00      1.00      1.00         6\n",
      "         103       1.00      1.00      1.00         6\n",
      "         104       1.00      1.00      1.00         6\n",
      "         105       0.83      0.83      0.83         6\n",
      "         106       0.50      0.83      0.62         6\n",
      "         107       1.00      1.00      1.00         6\n",
      "         108       1.00      1.00      1.00         6\n",
      "         109       1.00      0.83      0.91         6\n",
      "         110       1.00      0.83      0.91         6\n",
      "         111       1.00      1.00      1.00         6\n",
      "         112       1.00      1.00      1.00         6\n",
      "         113       0.86      1.00      0.92         6\n",
      "         114       0.86      1.00      0.92         6\n",
      "         115       1.00      1.00      1.00         6\n",
      "         116       1.00      1.00      1.00         6\n",
      "         117       1.00      1.00      1.00         6\n",
      "         118       1.00      1.00      1.00         6\n",
      "         119       1.00      1.00      1.00         6\n",
      "         120       1.00      1.00      1.00         6\n",
      "         121       1.00      1.00      1.00         6\n",
      "         122       1.00      1.00      1.00         6\n",
      "         123       1.00      1.00      1.00         6\n",
      "         124       1.00      1.00      1.00         6\n",
      "         125       1.00      1.00      1.00         6\n",
      "         126       1.00      1.00      1.00         6\n",
      "         127       1.00      1.00      1.00         6\n",
      "         128       1.00      1.00      1.00         6\n",
      "         129       1.00      1.00      1.00         6\n",
      "         130       0.86      1.00      0.92         6\n",
      "         131       1.00      1.00      1.00         6\n",
      "         132       1.00      1.00      1.00         6\n",
      "         133       1.00      1.00      1.00         6\n",
      "         134       1.00      1.00      1.00         6\n",
      "         135       1.00      1.00      1.00         6\n",
      "         136       1.00      1.00      1.00         6\n",
      "         137       1.00      1.00      1.00         6\n",
      "         138       1.00      1.00      1.00         6\n",
      "         139       1.00      1.00      1.00         6\n",
      "         140       1.00      0.83      0.91         6\n",
      "         141       1.00      1.00      1.00         6\n",
      "         142       1.00      1.00      1.00         6\n",
      "         143       0.86      1.00      0.92         6\n",
      "         144       1.00      1.00      1.00         6\n",
      "         145       1.00      1.00      1.00         6\n",
      "         146       1.00      1.00      1.00         6\n",
      "         147       1.00      1.00      1.00         6\n",
      "         148       1.00      1.00      1.00         6\n",
      "         149       1.00      1.00      1.00         6\n",
      "         150       1.00      1.00      1.00         6\n",
      "         151       1.00      1.00      1.00         6\n",
      "         152       1.00      1.00      1.00         6\n",
      "         153       1.00      1.00      1.00         6\n",
      "         154       1.00      1.00      1.00         6\n",
      "         155       1.00      1.00      1.00         6\n",
      "         156       0.71      0.83      0.77         6\n",
      "         157       1.00      1.00      1.00         6\n",
      "         158       1.00      0.83      0.91         6\n",
      "         159       1.00      1.00      1.00         6\n",
      "         160       1.00      1.00      1.00         6\n",
      "         161       1.00      0.67      0.80         6\n",
      "         162       1.00      0.83      0.91         6\n",
      "         163       0.75      1.00      0.86         6\n",
      "         164       1.00      1.00      1.00         6\n",
      "         165       1.00      1.00      1.00         6\n",
      "         166       0.67      0.67      0.67         6\n",
      "         167       0.86      1.00      0.92         6\n",
      "         168       1.00      1.00      1.00         6\n",
      "         169       1.00      1.00      1.00         6\n",
      "         170       1.00      1.00      1.00         6\n",
      "         171       1.00      0.83      0.91         6\n",
      "         172       1.00      1.00      1.00         6\n",
      "         173       1.00      1.00      1.00         6\n",
      "         174       1.00      0.83      0.91         6\n",
      "         175       1.00      1.00      1.00         6\n",
      "         176       1.00      1.00      1.00         6\n",
      "         177       1.00      1.00      1.00         6\n",
      "         178       1.00      1.00      1.00         6\n",
      "         179       1.00      1.00      1.00         6\n",
      "         180       1.00      1.00      1.00         6\n",
      "         181       1.00      1.00      1.00         6\n",
      "         182       1.00      1.00      1.00         6\n",
      "         183       1.00      1.00      1.00         6\n",
      "         184       1.00      1.00      1.00         6\n",
      "         185       1.00      1.00      1.00         6\n",
      "         186       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.96      1122\n",
      "   macro avg       0.96      0.96      0.96      1122\n",
      "weighted avg       0.96      0.96      0.96      1122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predictions, labels=list(label_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# save_dir = \"model/kobertLM_new_3_epoch18\"\n",
    "# model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load해서 쓸거면\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"model/kobertLM_new_3_epoch23\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 파라미터(변경시 꼭 수정해야해!)\n",
    "max_len = 122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "# 예시 #\n",
    "# 환불 때리고 싶은데 가능함?\n",
    "# 휴대폰으로 결제하고 싶은데 어떻게 해야하나요?\n",
    "# 비번 까먹었는데 어떻게 해야하나요?\n",
    "# 대전 성심당에 빵 뭐 있는지 알고 싶어요\n",
    "texts = [\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in texts:\n",
    "  encoded_dict = tokenizer.encode_plus(\n",
    "                          text,            \n",
    "                          add_special_tokens = True,\n",
    "                          max_length = max_len,\n",
    "                          pad_to_max_length = True, # padding =True 나 padding=\"longest\" 같은걸로 대체하세요\n",
    "                          return_attention_mask = True,\n",
    "                          return_tensors = 'pt')\n",
    "  \n",
    "  input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "  attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "  input_ids = torch.cat(input_ids, dim=0)\n",
    "  attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "model2.eval()\n",
    "\n",
    "predictions = []\n",
    "last_layer_attentions = []\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_masks = attention_masks.to(device)\n",
    "\n",
    "for i in range(len(input_ids)):\n",
    "\n",
    "  ids = input_ids[i].unsqueeze(0)\n",
    "  masks = attention_masks[i].unsqueeze(0)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model2(input_ids=ids,\n",
    "                      attention_mask=masks)\n",
    "\n",
    "  logits = outputs[0]\n",
    "  logits = torch.softmax(logits,dim=1)\n",
    "  last_layer_attention = outputs[1][-1]\n",
    "  \n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  last_layer_attention = last_layer_attention.detach().cpu().numpy()\n",
    "\n",
    "  last_layer_attentions.append(last_layer_attention)\n",
    "  predictions.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 환불하고싶어\n",
      "Prediction: 환불요청 (0.96)\n"
     ]
    }
   ],
   "source": [
    "probs = predictions[0][0]\n",
    "print(\"text:\", texts[0])\n",
    "pred_idx = np.argmax(probs)\n",
    "print(f\"Prediction: {intent_list[pred_idx]} ({probs[pred_idx]:.2f})\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [4.86108800e-03 9.30860732e-03 5.83432615e-04 4.94632288e-04\n",
      " 2.52864230e-03 4.51085670e-03 7.46212376e-04 1.50000374e-03\n",
      " 4.25411621e-03 4.64140021e-06 1.21743241e-02 5.52538596e-03\n",
      " 4.60196825e-05 2.44575064e-03 1.31486345e-03 9.87582374e-04\n",
      " 1.35614304e-04 1.21928228e-03 7.30044363e-08 7.53255546e-01\n",
      " 7.94948358e-03 1.61351042e-03 1.00046273e-04 9.36795899e-04\n",
      " 5.14498039e-04 2.78299376e-06 4.90002676e-05 1.44488840e-05\n",
      " 8.78564492e-07 1.52675057e-04 2.19308655e-04 7.23765173e-04\n",
      " 9.17103971e-05 6.71113139e-06 3.99483688e-05 2.34663108e-04\n",
      " 2.18029363e-05 8.02992872e-06 4.60721776e-06 1.28025877e-05\n",
      " 4.44860859e-07 1.61666117e-04 1.75775762e-03 2.47934368e-05\n",
      " 7.09049171e-04 1.59003917e-04 1.14525551e-06 3.93450428e-05\n",
      " 1.78237009e-04 6.60132901e-06 1.85203215e-04 1.36930448e-05\n",
      " 4.57918350e-06 9.16206161e-04 4.96171531e-04 6.95055423e-05\n",
      " 3.34485012e-05 2.48830853e-04 2.83344858e-03 1.74507426e-04\n",
      " 1.53111439e-04 3.69201007e-05 2.20334827e-04 3.17534577e-04\n",
      " 5.81522007e-04 1.15299318e-03 1.40506178e-04 5.99676023e-05\n",
      " 2.26844757e-04 1.60304844e-05 2.76282983e-04 2.15114724e-05\n",
      " 2.21758728e-05 4.66146194e-05 1.14991390e-05 3.58098769e-04\n",
      " 1.54723602e-04 3.27539419e-05 2.90168759e-06 3.44251348e-05\n",
      " 1.14680915e-05 1.83797704e-06 2.53797734e-05 1.30135857e-04\n",
      " 4.03561746e-04 7.09268897e-06 3.83517108e-05 1.21540004e-04\n",
      " 5.85105317e-06 3.86472902e-06 5.20954345e-05 6.27487498e-06\n",
      " 5.98118186e-06 9.82161437e-05 4.60061674e-05 5.62027143e-03\n",
      " 3.20663676e-05 3.13689146e-04 1.93965388e-05 1.24042690e-05\n",
      " 5.92268589e-05 1.84412147e-05 5.18056549e-05 1.32369503e-04\n",
      " 1.88333252e-05 1.55144560e-04 3.02366971e-04 1.16594972e-04\n",
      " 5.18688794e-05 7.71362276e-04 1.03103441e-04 1.37598079e-03\n",
      " 1.91171508e-04 2.11117458e-05 1.20841169e-05 7.96560107e-06\n",
      " 2.25282711e-05 3.72802460e-05 3.15646639e-06 1.51701233e-05\n",
      " 3.99734452e-03 1.62763020e-03 1.23935271e-04 1.12916760e-01\n",
      " 2.25280441e-04 1.57501490e-04 2.82881751e-06 1.77459500e-04\n",
      " 1.11855400e-06 3.35042423e-05 3.45177774e-04 2.73905443e-05\n",
      " 9.33336793e-04 6.21349609e-05 2.58637156e-05 7.09894175e-06\n",
      " 1.40073913e-04 2.76631927e-05 1.17025629e-04 2.14717829e-06\n",
      " 3.62691906e-04 8.48461582e-07 5.43983660e-05 8.83737175e-06\n",
      " 1.75901601e-04 6.54176329e-05 4.26759070e-05 5.03634283e-06\n",
      " 8.76485728e-07 5.98210590e-06 3.11694635e-07 2.12874647e-06\n",
      " 6.27141753e-06 2.85753458e-06 2.18053065e-05 5.57967780e-07\n",
      " 1.50934749e-04 2.31790218e-05 5.60849185e-05 2.11571176e-02\n",
      " 2.89399196e-07 1.55267553e-04 3.26789304e-05 1.04534047e-05\n",
      " 3.65865235e-05 6.70628924e-06 1.45096070e-04 6.85486293e-06\n",
      " 1.74336662e-07 1.01039295e-05 1.77425158e-04 5.32366394e-05\n",
      " 3.52844779e-07 8.31121815e-06 7.90703198e-05 1.32345769e-04\n",
      " 2.30521869e-06 9.09091432e-06 1.67048842e-04 1.66826270e-04\n",
      " 8.60843793e-05 3.39730008e-07 1.50187232e-04 1.90318897e-04\n",
      " 3.71675342e-05 1.03325707e-04 4.06231884e-05 1.38077405e-04\n",
      " 2.29495636e-05 1.62602901e-05 9.29808903e-06 1.21111598e-05\n",
      " 2.37305630e-06 2.61337595e-06 7.44271438e-06 1.94900596e-04\n",
      " 1.42373328e-05 3.89701199e-05 8.99245060e-06 6.71485122e-06\n",
      " 1.77547874e-04 6.40884082e-06 9.27895508e-06 5.39905304e-05\n",
      " 3.07445771e-05 1.10791577e-03 1.35414957e-05 4.12117079e-05\n",
      " 5.04664149e-06 9.96129093e-05 9.47597164e-06 1.23509439e-04\n",
      " 1.96762830e-05 5.59016516e-05 1.03512639e-05 1.80322604e-05\n",
      " 1.89185812e-05 8.68627467e-06 2.66040424e-05 9.26227731e-05\n",
      " 5.05994030e-05 3.63655272e-04 2.93546727e-05 2.20274742e-04\n",
      " 7.89830447e-05 3.96253745e-06 2.70322607e-06 9.00747523e-07\n",
      " 9.17687157e-06 2.30211572e-05 6.61512095e-05 3.71939859e-05\n",
      " 1.20519668e-04 5.35261188e-06 1.32781242e-05 2.11074468e-04\n",
      " 4.77848807e-04 3.01341988e-05 1.83003904e-05 9.11609095e-06\n",
      " 4.21867844e-06 7.75076915e-05 5.72846511e-05 6.54462856e-05\n",
      " 3.57393702e-07 6.03328044e-06 2.12613941e-06 3.84876967e-06\n",
      " 8.86169000e-05 4.49342660e-05 2.82726269e-05 3.77789661e-06\n",
      " 4.09581025e-06 7.86068267e-06 3.56277724e-06 1.04847741e-05\n",
      " 1.42625902e-06 2.42608239e-05 4.66921265e-05 9.42673978e-06\n",
      " 8.24915551e-05 2.59079563e-04 2.10809048e-05 5.73467514e-05\n",
      " 3.73615883e-04 1.18057818e-04 5.18992638e-05 1.69427528e-06\n",
      " 2.21514665e-06 1.39906217e-06 1.32701989e-05 2.19966983e-04\n",
      " 2.37389861e-04 3.97607619e-05 8.09091762e-06 3.39324943e-05\n",
      " 1.70040294e-05 1.22794127e-05 1.40082600e-04 1.28852680e-05\n",
      " 1.65773108e-05 2.31351842e-05 1.03332794e-04 4.68574835e-05\n",
      " 1.18075095e-05 1.44293990e-05 1.73404460e-05 8.89453295e-05\n",
      " 1.50215885e-06 4.07187372e-06 2.40868496e-04 1.10497545e-04\n",
      " 3.55893135e-05 4.38390634e-05 2.46860145e-05 2.12552204e-05\n",
      " 1.15235334e-05 8.20262067e-05 2.65815961e-05 1.46040975e-05\n",
      " 5.98824181e-06 1.45567697e-06 2.56182811e-05 1.36630246e-04\n",
      " 1.14124168e-05 1.30411099e-05 3.32616332e-06 1.26493615e-04\n",
      " 2.36680826e-05 5.94522317e-06 1.38101245e-06 2.07922494e-05\n",
      " 4.35596121e-06 5.04395430e-05 4.27603780e-04 1.05645997e-03\n",
      " 1.76738671e-04 6.81715901e-05 2.22229773e-05 5.61034540e-06\n",
      " 4.73550172e-05 1.78222963e-05 3.69034860e-05 4.65307603e-05\n",
      " 3.63965119e-05 2.52137806e-06 1.79421513e-05 7.29074463e-06\n",
      " 1.11139256e-04 1.06496067e-04 1.92811240e-05 2.54944553e-06\n",
      " 5.91782191e-06 1.83273060e-05 1.87251819e-04 6.91618743e-06\n",
      " 3.77071774e-05 1.11463419e-06 2.27730961e-05 3.34961737e-06\n",
      " 5.46727370e-06 2.36148185e-06 1.34065340e-05 6.52621675e-05\n",
      " 1.86158832e-05 2.40819986e-06 2.24820701e-06 1.17130203e-05\n",
      " 5.47758355e-05 9.51036509e-06 3.28459191e-06 5.02771172e-07\n",
      " 2.39938585e-04 1.33894009e-05 2.97602514e-06 1.82527106e-06\n",
      " 1.64119847e-05 2.24334217e-05 7.58364149e-06 4.81685929e-05\n",
      " 2.92262066e-05 8.04216779e-06 7.65860295e-06 8.19339475e-05\n",
      " 7.28931218e-06 1.14412023e-06 2.73868477e-06 3.30226567e-06\n",
      " 9.48399611e-05 4.36860428e-04 3.25856345e-05 2.13580686e-04\n",
      " 5.87120048e-05 3.39400822e-06 8.58421728e-04 1.59198044e-05\n",
      " 1.66412719e-05 5.35232430e-05 3.77950055e-04 1.46378825e-05\n",
      " 1.88384904e-04 1.79681524e-06 2.86785307e-05 2.35585921e-05\n",
      " 3.48539543e-05 1.51732347e-05 2.80591394e-05 9.59791760e-06\n",
      " 1.24710787e-04 4.07766447e-06 1.83326010e-05 3.10904325e-05\n",
      " 3.19088977e-05 1.76810147e-06 7.67370784e-06 2.08724441e-06\n",
      " 1.34918810e-05 6.73182001e-07 5.54384633e-06 1.21927292e-06\n",
      " 7.18859701e-06 1.43413372e-05 2.45975252e-06 4.58225441e-06\n",
      " 4.78146205e-07 1.79384715e-05 1.86389952e-05 4.74704024e-07\n",
      " 4.04544335e-05 5.89145493e-05 6.70544105e-05 4.12140289e-05\n",
      " 3.28277747e-05 1.24760454e-05 3.11672338e-05 1.83455355e-04]\n"
     ]
    }
   ],
   "source": [
    "# ### 전체 pred 값 ###\n",
    "# print(\"predictions:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66793dee8b4a2983b6dd1bee8f592930418cb9cc68c57734dfcad44387fdcccf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
